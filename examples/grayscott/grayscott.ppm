client grayscott
 
  use ppm_module_util_hsort

  integer, dimension(6) :: bcdef = ppm_param_bcdef_periodic
  integer, dimension(2) :: seed
  real(ppm_kind_double), dimension(:,:), pointer :: displace
  real(ppm_kind_double) :: noise = 0.0_mk
  integer               :: istage = 1
  integer               :: interval
  real(ppm_kind_double) :: time_diff
  
  real(mk), dimension(:,:), pointer :: minsub => null()
  real(mk), dimension(:,:), pointer :: maxsub => null()
  integer, dimension(:), pointer    :: sub2proc => null() 
  integer                           :: nsublist
  type(ppm_t_particles_d), pointer :: c

  global_var(st,integer,0)
  global_var(iter_time,integer,0)
  global_var(dcop_time,integer,0)
  global_var(map_time,integer,0)

  add_arg(k_rate,<#real(mk)#>,1.0_mk,0.0_mk,'k_rate','Reaction rate')
  add_arg(F,<#real(mk)#>,1.0_mk,0.0_mk,'F_param','Reaction parameter F')
  add_arg(D_u,<#real(mk)#>,1.0_mk,0.0_mk,'Du_param','Diffusion constant of U')
  add_arg(D_v,<#real(mk)#>,1.0_mk,0.0_mk,'Dv_param','Diffusion constant of V')
  add_arg(npart_global,<#integer#>,1,1,ctrl_name='npart',help_txt='global number of particles')
  
  add_arg(subs,<#integer,dimension(2)#>,<#(/1,1/)#>,ctrl_name='subs',help_txt='number of subdomains in each direction')
  add_arg(pernode,<#integer,dimension(2)#>,<#(/1,1/)#>,ctrl_name='pernode',help_txt='number of subdomains in each direction')
  ppm_init()

  U = create_field(1, "U")
  V = create_field(1, "V")


  ! creating a decomposition and assignment
  ! specifically for the benchmarks
  ! ---------------------------------------

  if (nproc.ge.4) then
  call mk_cart_topo(min_phys,max_phys,subs,pernode,minsub,maxsub,&
  &                 nsublist,sub2proc,info)


  topo = create_topology(bcdef,decomp=ppm_param_decomp_user_defined,assign=ppm_param_assign_user_defined,minsl=minsub,maxsl=maxsub,nsl=nsublist,s2p=sub2proc)

  else
    ! the boring case
    topo = create_topology(bcdef)
  endif

  !c = create_particles(topo)
  call init_parts(topo,npart_global,ghost_size,c)
  set_type(c,ppm_t_particles_d)
  
  allocate(displace(ppm_dim,c%Npart))
  call random_number(displace)
  displace = (displace - 0.5_mk) * c%h_avg * 0.15_mk
  call c%move(displace, info)
  call c%apply_bc(info)
  global_mapping(c, topo)


  ! Sorting the particles according to a Hilbert curve
  call ppm_hsort(c,info)
  

  discretize(U,c)
  discretize(V,c)

  map_ghost_get(c)

  foreach p in particles(c) with positions(x) sca_fields(U,V)
    U_p = 1.0_mk
    V_p = 0.0_mk 
    if (((x_p(1) - 5.0_mk)**2 + (x_p(2) - 5.0_mk)**2) .lt. 0.1) then
      call random_number(noise)
      U_p = 0.5_mk  + 0.01_mk*noise
      call random_number(noise)
      V_p = 0.25_mk + 0.01_mk*noise
    end if
  end foreach
  
  n = create_neighlist(c,cutoff=<#2.5_mk * c%h_avg#>)
  if (ppm_dim .eq. 2) then
    Lap = define_op(2, [2,0, 0,2], [1.0_mk, 1.0_mk], "Laplacian")
  else 
    Lap = define_op(3, [2,0,0, 0,2,0, 0,0,2], [1.0_mk, 1.0_mk, 1.0_mk], "Laplacian")
  end if 

  L = discretize_op(Lap, c, ppm_param_op_dcpse,[order=>2,c=>1.0_mk])

  o, nstages = create_ode([U,V], grayscott_rhs, [U=>c,V], eulerf)
  !interval = int(stop_time/time_step)/1000
  
  call ppm_tstats_setup(1,info)
  !call ppm_tstats_add('g_map',map_time,info)
  !call ppm_tstats_add('dcop',dcop_time,info)
  call ppm_tstats_add('iteration',iter_time,info)
  
  t = timeloop()
    call ppm_tstats_tic(iter_time,st+1,info) 
    do istage=1,nstages
      !call ppm_tstats_tic(map_time,st+1,info) 
      map_ghost_get(c,psp=true)
      !call ppm_tstats_toc(map_time,st+1,time_diff,info) 
      
      ode_step(o, t, time_step, istage)
    end do
    st = st + 1
    call ppm_tstats_toc(iter_time,st,time_diff,info) 
    if (ppm_rank.eq.0) print *,st
  end timeloop
  !print([U=>c, V=>c],1 )
  call ppm_tstats_collect('time.dat',info)
  ppm_finalize()

  contains


  subroutine init_parts(tid,npart_global,cutoff,parts)

  use ppm_module_core
  implicit none
  integer, intent(in) :: tid
  integer, intent(in) :: npart_global
  type(ppm_t_particles_d), pointer :: parts
  real(mk), intent(in) :: cutoff

  real(mk), dimension(:,:), pointer :: xp
  type(ppm_t_topo), pointer :: topo
  integer :: i,j,ip,ni,nj
  integer :: isub,sub_idx
  real(mk), dimension(ppm_dim) :: len_phys,len_sub
  real(mk) :: h
  real(mk) :: x,y
  integer              :: npart

  topo => ppm_topo(tid)%t

  len_phys = max_phys - min_phys

  h = (PRODUCT(len_phys)/REAL(npart_global))**(1.0_mk/real(ppm_dim))

  npart = 0
  do isub = 1,topo%nsublist
    sub_idx = topo%isublist(isub)
    len_sub = topo%max_subd(:,sub_idx) - topo%min_subd(:,sub_idx)
    ni = nint(len_sub(1)/h)
    nj = nint(len_sub(2)/h)
    npart = npart + ni*nj
  enddo
  allocate(parts,stat=info)
 
  call parts%create(npart,info)
  parts%active_topoid = tid
  call parts%get_xp(xp,info)
  parts%ghostlayer = cutoff
  parts%flags(ppm_part_cartesian) = .true.
  parts%flags(ppm_part_areinside) = .true.
  xp => parts%xp

  ip = 0
  do isub = 1,topo%nsublist
    sub_idx = topo%isublist(isub)
    len_sub = topo%max_subd(:,sub_idx) - topo%min_subd(:,sub_idx)
    ni = nint(len_sub(1)/h)
    nj = nint(len_sub(2)/h)
    do j=1,nj
      h = len_sub(2)/REAL(nj,MK)
      y = topo%min_subd(2,sub_idx) + h*(j-1)
      do i=1,ni
        h = len_sub(1)/REAL(ni,MK)
        x = topo%min_subd(1,sub_idx) + h*(i-1)
        ip = ip + 1
        xp(1,ip) = x
        xp(2,ip) = y
      enddo
    enddo
  enddo

  if (ip.ne.npart) print *,rank,'ip != npart',ip,npart

  parts%h_avg = (PRODUCT(len_phys)/REAL(npart_global))**(1./REAL(ppm_dim))
  call parts%set_xp(xp,info)

  end subroutine



  subroutine mk_cart_topo(min_phys,max_phys,subs,pernode,minsub,maxsub,&
                          nsublist,sub2proc,info)
  implicit none 
  include 'mpif.h'
  integer, parameter :: mk = 8 ! precision
 
  ! input arguments
  integer, dimension(2), intent(in) :: pernode
  integer, dimension(2) ,intent(in):: subs
  real(mk), dimension(2),intent(in) :: min_phys,max_phys
  
  ! output arguments 
  real(mk), dimension(:,:), pointer :: minsub => null()
  real(mk), dimension(:,:), pointer :: maxsub => null()
  integer, dimension(:), pointer    :: sub2proc => null() 
  integer              ,intent(out) :: nsublist
  integer              ,intent(out) :: info
 
  ! variables
  integer :: nproc,comm,rank
  integer :: slen
  integer :: i,j,k,l,idx,ii,jj
  integer :: nnodes
  character*(MPI_MAX_PROCESSOR_NAME) :: host
  character*(MPI_MAX_PROCESSOR_NAME), dimension(:), pointer :: allhosts => null()
  integer, dimension(:,:), pointer :: hostonnode => null()
  integer, dimension(:,:), pointer :: cart2sub => null()
  real(mk) :: subx,suby


  ! subdomains must be all square and the domain decomposition should
  ! produce a n x m cartesian decomposition
  ! the user has to provide subsn and subsm --> subs(1),subs(2)
  ! subsn*subsm should be equal to nproc and 
  !   maxphysx-minphysx ~ subsn
  !   maxphysy-minphysy ~ subsm
  subx = (max_phys(1)-min_phys(1))/subs(1)
  suby = (max_phys(2)-min_phys(2))/subs(2)
  allocate(minsub(2,product(subs)),maxsub(2,product(subs)),&
  &        cart2sub(subs(1),subs(2)),stat=info)
  nsublist = product(subs)
  do j=1,subs(2) ! <-- y direction of domain
    do i=1,subs(1) ! <-- x direction of domain
      minsub(1,(j-1)*subs(1)+i) = (i-1)*subx
      minsub(2,(j-1)*subs(1)+i) = (j-1)*suby
      maxsub(1,(j-1)*subs(1)+i) = i*subx
      maxsub(2,(j-1)*subs(1)+i) = j*suby
      cart2sub(i,j) = (j-1)*subs(1)+i
    enddo
  enddo

  ! sub to proc assignment:
  ! since each proc has exactly one sub and we are using a cartesian
  ! decomp we should try to put subs that are geometrically close also
  ! on procs that are close
  
  comm = MPI_COMM_WORLD
  
  call MPI_Comm_Size(comm, nproc, info)
  call MPI_Comm_Rank(comm, rank, info)

  nnodes = nproc / product(pernode)
  
  ! ask mpi about this ranks hostname
  call MPI_Get_processor_name(host,slen,info)

  print *,rank,host(1:slen)

  allocate(allhosts(nproc),stat=info)
  ! get the hostnames from all ranks
  call MPI_Gather(host,MPI_MAX_PROCESSOR_NAME,MPI_CHARACTER,&
  &               allhosts,MPI_MAX_PROCESSOR_NAME,MPI_CHARACTER,0,comm,info)
 
  allocate(sub2proc(nsublist),stat=info) 
  ! do the assignment on rank0
  if (rank.eq.0) then 
    !do i=1,nproc
    !  print*,allhosts(i)(1:slen)
    !enddo
    allocate(hostonnode(product(pernode),nnodes))
    call groupby(allhosts,nproc,product(pernode),hostonnode)
    
    print *,'------'
    do i=1,nnodes
      print *,i,'--',hostonnode(:,i)
    enddo
    do j = 1,subs(2)/pernode(2)
      jj = (j-1)*pernode(2) + 1
      do i = 1,subs(1)/pernode(1)
        ii = (i-1)*pernode(1) + 1
        do k = 0,pernode(2) - 1
          do l = 0,pernode(1) - 1
            idx = cart2sub(ii+l,jj+k)
            sub2proc(idx) = hostonnode(k*pernode(1)+(l+1), &
            &                          (j-1)*(subs(1)/pernode(1))+i)
          enddo
        enddo
      enddo
    enddo
    print *,'------'
    do i=1,nsublist
      print *,i,sub2proc(i),minsub(:,i)
    enddo
  endif

  ! now tell all ranks about the assignment
  call MPI_Bcast(sub2proc, nsublist, MPI_INTEGER, 0, comm, info)
  end subroutine

  subroutine groupby(hosts,nhosts,npernode,grouped)
  implicit none
  include 'mpif.h'
  character*(MPI_MAX_PROCESSOR_NAME), dimension(nhosts),intent(in) :: hosts
  integer, intent(in) :: nhosts
  integer, intent(in) :: npernode
  integer, dimension(npernode,nhosts/npernode) :: grouped
  integer :: i,hind,hi,ni
  integer, dimension(nhosts) :: ranks

  do i = 1,nhosts
    ranks(i) = i - 1  
  enddo
  ni = 1
  hind = 1
  do while (hind <= nhosts)
    if (ranks(hind) > -1) then
      hi = 1
      do i = hind, nhosts
        if (hosts(i) == hosts(hind)) then
          grouped(hi,ni) = ranks(i)
          ranks(i) = -1
          hi = hi + 1
        endif
      enddo
      ni = ni + 1
    endif
    hind = hind + 1
  enddo



  end subroutine groupby
  
  
  
  
end client

rhs grayscott_rhs(U=>parts,V)
  real(mk) :: tdiff 
  get_fields(dU,dV)

  !call ppm_tstats_tic(dcop_time,st+1,info) 
  dU = apply_op(L, U)
  dV = apply_op(L, V)
  !call ppm_tstats_toc(dcop_time,st+1,tdiff,info) 


  foreach p in particles(parts) with sca_fields(U,V,dU,dV)
    dU_p = D_u*dU_p - U_p*(V_p**2) + F*(1.0_mk-U_p)
    dV_p = D_v*dV_p + U_p*(V_p**2) - (F+k_rate)*V_p
  end foreach

end rhs
